{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e559cff8",
   "metadata": {},
   "source": [
    "### 1. Define Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c945b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e748c24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatbotModel(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(ChatbotModel, self).__init__()\n",
    "\n",
    "        #  fc1: รับข้อมูลจากคลังคำศัพท์ (input_size) แล้วขยาย/แปลงเป็น 128 มิติ\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "\n",
    "        # fc2: บีบข้อมูลจาก 128 เหลือ 64 มิติ เพื่อสรุปใจความสำคัญ\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "\n",
    "        # fc3: ตัวตัดสินใจสุดท้าย โดยส่งค่าออกไปตามจำนวนหมวดหมู่คำตอบที่มี (output_size)\n",
    "        self.fc3 = nn.Linear(64, output_size)\n",
    "\n",
    "        #  relu: ตัวช่วยให้บอทเข้าใจความซับซ้อนของภาษา (Non-linearity)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # dropout: ตัวช่วยกันบอท \"ท่องจำ\" โดยสุ่มปิดการทำงานของเซลล์ประสาท 50% ในขณะเทรน PyTorch Dropout\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        # Input -> fc1 -> ReLU: ข้อมูลถูกส่งเข้าเลเยอร์แรกและแปลงค่าด้วย ReLU เพื่อหาความสัมพันธ์ของคำ\n",
    "        X = self.relu(self.fc1(X))\n",
    "\n",
    "        # Dropout: สุ่มปิดสัญญาณบางส่วน (เฉพาะตอนเทรน) เพื่อให้โมเดลมีความยืดหยุ่น\n",
    "        X = self.dropout(X)\n",
    "\n",
    "        # fc2 -> ReLU: ประมวลผลข้อมูลในระดับที่ลึกขึ้นเพื่อระบุเอกลักษณ์ของประโยค\n",
    "        X = self.relu(self.fc2(X))\n",
    "\n",
    "        # Dropout: ทำซ้ำอีกครั้งเพื่อความเสถียร\n",
    "        X = self.dropout(X)\n",
    "\n",
    "        # fc3: ส่งผลลัพธ์เป็นคะแนน (Raw scores/Logits) ของแต่ละ Intent ออกไป\n",
    "        X = self.fc3(X)\n",
    "\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb634939",
   "metadata": {},
   "source": [
    "- ข้อมูลจะถูกส่งเข้าเป็นตัวเลข (Bag of Words) -> ถูกบีบอัดและวิเคราะห์ผ่าน 3 เลเยอร์ -> ผลลัพธ์สุดท้ายคือ คะแนนความมั่นใจ ในแต่ละ Tag (เช่น เป็นการทักทาย 90%, เป็นการถามราคา 5%) เพื่อให้ฟังก์ชัน process_message เลือกคำตอบที่แม่นยำที่สุดมาแสดงครับ PyTorch nn.Module Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be3be9f",
   "metadata": {},
   "source": [
    "### 2. Define Manager Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a13e9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatbotAssistant:\n",
    "\n",
    "    def __init__(self, intents_path, function_mappings = None):\n",
    "\n",
    "        # self.model = None: จองที่ไว้สำหรับเก็บตัวโมเดล Neural Network (PyTorch) \n",
    "        # ซึ่งจะถูกสร้างขึ้นภายหลังในขั้นตอนการเทรนหรือการโหลดไฟล์ PyTorch nn.Module Reference\n",
    "        self.model = None\n",
    "\n",
    "        # self.intents_path = intents_path: เก็บที่อยู่ (Path) ของไฟล์ JSON \n",
    "        # ที่บรรจุโครงสร้างคำถาม-คำตอบ (เช่น intents.json)\n",
    "        self.intents_path = intents_path\n",
    "\n",
    "        # self.documents = []: เก็บรายการคู่ของ (รายการคำศัพท์, แท็ก) \n",
    "        # เช่น ([\"hello\"], \"greeting\") เพื่อใช้เป็นฐานข้อมูลในการเทรน\n",
    "        self.documents = []\n",
    "\n",
    "        # self.vocabulary = []: เก็บ คำศัพท์ทั้งหมดแบบไม่ซ้ำกัน ที่บอทรู้จัก \n",
    "        # (ใช้สร้าง Bag of Words)\n",
    "        self.vocabulary = []\n",
    "\n",
    "        # self.intents = []: เก็บรายชื่อ Tag หรือหมวดหมู่ ทั้งหมด \n",
    "        # (เช่น \"greeting\", \"goodbye\", \"thanks\")\n",
    "        self.intents = []\n",
    "\n",
    "        # self.intents_response = {}: ดิกชันนารีที่เก็บ รายการคำตอบ ของแต่ละ Tag \n",
    "        # เพื่อให้บอทสุ่มดึงไปตอบผู้ใช้ได้รวดเร็ว\n",
    "        self.intents_response = {}\n",
    "\n",
    "        # (Optional) เป็นส่วนที่ใช้เชื่อมต่อ Intent เข้ากับฟังก์ชันในโปรแกรม\n",
    "        self.function_mappings = function_mappings\n",
    "\n",
    "        # จองที่ไว้เก็บ Features (ข้อมูล Input ในรูปตัวเลข 0, 1 จาก Bag of Words)\n",
    "        self.X = None\n",
    "\n",
    "        # จองที่ไว้เก็บ Labels (ตัวเลขดัชนีของแต่ละ Intent)\n",
    "        self.y = None\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenize_lemmatize(text):\n",
    "\n",
    "        # แปลงคำให้กลับไปเป็น \"รากศัพท์\"\n",
    "        lemmatizer = nltk.WordNetLemmatizer()\n",
    "\n",
    "        # Tokenization: คือการตัดประโยคยาวๆ ออกเป็นคำย่อยๆ (Tokens)\n",
    "        words = nltk.word_tokenize(text)\n",
    "\n",
    "        # นำคำที่ได้ไปหาต้นแบบรากศัพท์ตามที่เตรียมไว้\n",
    "        words = [lemmatizer.lemmatize(word.lower()) for word in words]\n",
    "\n",
    "        return words\n",
    "    \n",
    "    # bag_of_words คือขั้นตอนการทำ Feature Extraction เพื่อเปลี่ยน \"รายการคำศัพท์\" (ซึ่งคอมพิวเตอร์คำนวณไม่ได้) \n",
    "    # ให้กลายเป็น \"ตัวเลข (Vector)\" ที่มีความยาวคงที่ เพื่อส่งให้ Neural Network\n",
    "    def bag_of_words(self, words):\n",
    "\n",
    "        # self.vocabulary: คือคลังคำศัพท์ทั้งหมดที่บอทเคยเรียนรู้มาจากไฟล์ JSON (เช่น ถ้าบอทรู้จักคำทั้งหมด 100 คำ ลิสต์นี้จะมีสมาชิก 100 ตัว)\n",
    "        # for word in self.vocabulary: คือการวนลูปตรวจสอบทีละคำในคลังคำศัพท์ทั้งหมดตามลำดับ\n",
    "        # return One-hot Encoding หรือ Bag of Words Vector\n",
    "\n",
    "        return [1 if word in words else 0 for word in self.vocabulary]\n",
    "    \n",
    "    # เปลี่ยนข้อมูลจากไฟล์ JSON ให้กลายเป็นโครงสร้างข้อมูลที่พร้อมสำหรับการฝึกฝนโมเดล\n",
    "    def parse_intents(self):\n",
    "        lemmatizer = nltk.WordNetLemmatizer()\n",
    "\n",
    "        if os.path.exists(self.intents_path):\n",
    "            with open(self.intents_path, 'r') as f:\n",
    "                intents_data = json.load(f)\n",
    "\n",
    "            for intent in intents_data['intents']:\n",
    "                if intent['tag'] not in self.intents:\n",
    "\n",
    "                    # เก็บชื่อหมวดหมู่ (Tag) ทั้งหมดไว้ในลิสต์ เช่น [\"greeting\", \"goodbye\"]\n",
    "                    self.intents.append(intent['tag'])\n",
    "\n",
    "                    # สร้างฐานข้อมูลคำตอบไว้ในดิกชันนารี เพื่อให้บอทเรียกใช้ได้ทันทีเมื่อจำแนก Tag ได้แล้ว\n",
    "                    self.intents_response[intent['tag']] = intent['responses']\n",
    "\n",
    "                for pattern in intent['patterns']:\n",
    "\n",
    "                    # นำประโยคมาตัดคำและแปลงเป็นรากศัพท์ \n",
    "                    pattern_words = self.tokenize_lemmatize(pattern)\n",
    "\n",
    "                    # เก็บคำศัพท์ใหม่ๆ ที่เจอเพิ่มเข้าไปในคลังคำศัพท์รวม\n",
    "                    self.vocabulary.extend(pattern_words)\n",
    "\n",
    "                    # สร้างคู่หูข้อมูลเก็บไว้ในรูปแบบ (รายการคำศัพท์, ชื่อ Tag) \n",
    "                    # เพื่อบอกโมเดลว่า \"ถ้าเจอคำกลุ่มนี้ ให้แปลว่าคือ Tag นี้\"\n",
    "                    self.documents.append((pattern_words, intent['tag']))\n",
    "\n",
    "                # เพื่อให้ลำดับของคำใน Bag of Words คงที่เสมอ (เช่น ตำแหน่งที่ 0 คือคำว่า 'apple' เสมอ)\n",
    "                self.vocabulary = sorted(set(self.vocabulary))\n",
    "\n",
    "    # แปลงข้อมูลจาก \"คู่คำศัพท์และหมวดหมู่\" ให้กลายเป็น \"ชุดตัวเลขเชิงคณิตศาสตร์\" (Numerical Tensors)\n",
    "    def prepare_data(self):\n",
    "\n",
    "        # bags = []: ลิสต์สำหรับเก็บ Vector ของคำศัพท์ (ข้อมูลนำเข้า หรือ Input/Features)\n",
    "        bags = []\n",
    "\n",
    "        # indices = []: ลิสต์สำหรับเก็บตัวเลขดัชนีของหมวดหมู่ (คำเฉลย หรือ Output/Labels)\n",
    "        indices = []\n",
    "\n",
    "        for document in self.documents:\n",
    "\n",
    "            # words = document[0]: ดึงรายการคำศัพท์ที่ตัดแล้วออกมา\n",
    "            words = document[0]\n",
    "\n",
    "            # bag = self.bag_of_words(words): เรียกฟังก์ชัน bag_of_words เพื่อเปลี่ยนรายการคำให้เป็นแถวของตัวเลข 0 และ 1 \n",
    "            # (เช่น [0, 1, 0, 0, 1])\n",
    "            bag = self.bag_of_words(words)\n",
    "\n",
    "            # ทำ Label Encoding โดยการเปลี่ยนชื่อ Tag (เช่น \"greeting\") ให้กลายเป็นตัวเลขดัชนี\n",
    "            intent_index = self.intents.index(document[1])\n",
    "\n",
    "            # bags.append(bag) และ indices.append(intent_index): เก็บค่าที่แปลงได้ลงในลิสต์รวม\n",
    "            bags.append(bag)\n",
    "            indices.append(intent_index)\n",
    "\n",
    "        # self.X = np.array(bags): แปลงลิสต์ของ Bag of Words ทั้งหมดให้เป็น Matrix ขนาดใหญ่ เพื่อใช้เป็นตัวแปรต้น (X)\n",
    "        self.X = np.array(bags)\n",
    "\n",
    "        # self.y = np.array(indices): แปลงลิสต์ของดัชนีให้เป็น Vector ของตัวเลขเฉลย เพื่อใช้เป็นตัวแปรตาม (y)\n",
    "        self.y = np.array(indices)\n",
    "\n",
    "    # train_model คือขั้นตอนการสอนบอทให้เรียนรู้ (Training Phase) โดยนำข้อมูลตัวเลขที่เตรียมไว้มาผ่านกระบวนการทางคณิตศาสตร์\n",
    "    # เพื่อให้โมเดลจดจำความสัมพันธ์ระหว่างคำถามและคำตอบ\n",
    "    def train_model(self, batch_size, lr, epochs):\n",
    "\n",
    "        # torch.tensor(...): แปลงข้อมูลจาก NumPy ให้เป็น PyTorch Tensors \n",
    "        # เพื่อให้สามารถประมวลผลด้วย GPU หรือคำนวณ Gradient ได้\n",
    "        X_tensor = torch.tensor(self.X, dtype=torch.float32)\n",
    "        y_tensor = torch.tensor(self.y, dtype=torch.long)\n",
    "\n",
    "        # TensorDataset: นำ X (คำถาม) และ y (เฉลย) มามัดรวมกันเป็นคู่ๆ\n",
    "        dataset = TensorDataset(X_tensor, y_tensor)\n",
    "\n",
    "        # DataLoader: ตัวจัดการการป้อนข้อมูล โดยจะแบ่งข้อมูลเป็นกลุ่มเล็กๆ (batch_size) \n",
    "        # และทำการสลับข้อมูล (shuffle) ในทุกรอบ เพื่อให้โมเดลไม่จำลำดับการป้อนข้อมูล PyTorch DataLoader Documentation\n",
    "        loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        # self.model = ChatbotModel(...): สร้างโครงสร้าง Neural Network โดยกำหนดขนาด \n",
    "        # Input ตามจำนวนคำศัพท์ที่มี และ Output ตามจำนวนหมวดหมู่ (Intents)\n",
    "        self.model = ChatbotModel(self.X.shape[1], len(self.intents))\n",
    "\n",
    "        # criterion = nn.CrossEntropyLoss(): ฟังก์ชันวัดค่าความผิดพลาด (Loss Function) \n",
    "        # สำหรับงานจำแนกประเภท (Classification)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # optimizer = optim.Adam(...): อัลกอริทึมที่จะทำหน้าที่ปรับค่าน้ำหนักของโมเดล โดยใช้ค่า lr (Learning Rate) \n",
    "        # เป็นตัวกำหนดความเร็วในการก้าวเดิน Adam Optimizer Reference\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            running_loss = 0.0\n",
    "\n",
    "            for batch_X, batch_y in loader:\n",
    "\n",
    "                # optimizer.zero_grad(): ล้างค่าความผิดพลาดเก่าทิ้งก่อนเริ่มรอบใหม่\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # outputs = self.model(batch_X): ให้บอทลองทายคำตอบจากข้อมูลที่ได้รับ (Forward Pass)\n",
    "                outputs = self.model(batch_X)\n",
    "\n",
    "                # loss = criterion(outputs, batch_y): คำนวณดูว่าสิ่งที่บอททาย ต่างจากเฉลยมากน้อยแค่ไหน\n",
    "                loss = criterion(outputs, batch_y)\n",
    "\n",
    "                # loss.backward(): คำนวณทิศทางที่ต้องปรับปรุงค่าน้ำหนัก (Backpropagation)\n",
    "                loss.backward()\n",
    "\n",
    "                # optimizer.step(): ลงมือปรับค่าน้ำหนักในโมเดลจริงๆ\n",
    "                optimizer.step()\n",
    "\n",
    "                # running_loss += loss: สะสมค่าความผิดพลาดเพื่อนำมาพิมพ์รายงานผลในบรรทัดสุดท้าย\n",
    "                running_loss += loss\n",
    "\n",
    "            print(f\"Epoch {epoch+1}: Loss: {running_loss / len(loader):.4f}\")\n",
    "\n",
    "    def save_model(self, model_path, dimension_path):\n",
    "\n",
    "        # self.model.state_dict(): คำสั่งนี้จะดึงเฉพาะค่าน้ำหนัก (Weights) \n",
    "        # และค่า Bias ทั้งหมดที่โมเดลฝึกฝนจนสำเร็จออกมาในรูปแบบ Dictionary\n",
    "        torch.save(self.model.state_dict(), model_path)\n",
    "\n",
    "        with open(dimension_path, 'w') as f:\n",
    "            json.dump({ 'input_size': self.X.shape[1], 'output_size': len(self.intents) }, f)\n",
    "\n",
    "            # 'input_size': self.X.shape[1]: บันทึกว่าบอทตัวนี้รู้จักคำศัพท์ทั้งหมดกี่คำ (ขนาดของ Vocabulary)\n",
    "            # 'output_size': len(self.intents): บันทึกว่าบอทมีหมวดหมู่คำตอบทั้งหมดกี่หมวดหมู่\n",
    "            # json.dump(...): บันทึกค่าทั้งสองนี้ลงในไฟล์ JSON แยกต่างหาก\n",
    "\n",
    "    def load_model(self, model_path, dimension_path):\n",
    "        with open(dimension_path, 'r') as f:\n",
    "            dimensions = json.load(f)\n",
    "\n",
    "        # self.model = ChatbotModel(...): สร้างออบเจกต์โมเดลตัวเปล่าๆ ขึ้นมาตามขนาดที่อ่านได้จากไฟล์ Metadata\n",
    "        self.model = ChatbotModel(dimensions['input_size'], dimensions['output_size'])\n",
    "\n",
    "        self.model.load_state_dict(torch.load(model_path, weights_only=True))\n",
    "        # torch.load(model_path, weights_only=True): โหลดค่าน้ำหนักที่บันทึกไว้ในไฟล์ .pth\n",
    "        # self.model.load_state_dict(...): นำค่าน้ำหนักที่โหลดมาได้ ใส่เข้าไปในแต่ละเลเยอร์ของโมเดลตัวเปล่าที่เราเพิ่งสร้างขึ้น\n",
    "\n",
    "    def process_message(self, input_message):\n",
    "\n",
    "        # words = self.tokenize_lemmatize(input_message): นำประโยคที่คนพิมพ์มาตัดเป็นคำและหาต้นแบบรากศัพท์ \n",
    "        words = self.tokenize_lemmatize(input_message)\n",
    "\n",
    "        # bag = self.bag_of_words(words): แปลงลิสต์คำศัพท์ให้กลายเป็น Vector ของตัวเลข 0 และ 1 ตามคลังศัพท์ที่บอทรู้จัก\n",
    "        bag = self.bag_of_words(words)\n",
    "\n",
    "        # bag_tensor = torch.tensor([bag], ...): แปลงข้อมูลเป็น PyTorch Tensor เพื่อส่งเข้าโมเดล\n",
    "        bag_tensor = torch.tensor([bag], dtype=torch.float32)\n",
    "\n",
    "        # self.model.eval(): เปลี่ยนโมเดลเข้าสู่ \"โหมดใช้งานจริง\" เพื่อปิดการทำงานของเลเยอร์สุ่มอย่าง Dropout PyTorch model.eval()\n",
    "        self.model.eval()\n",
    "\n",
    "        # with torch.no_grad():: สั่งไม่ให้คำนวณ Gradient เพื่อประหยัด RAM และทำให้บอทตอบกลับเร็วขึ้น\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # predictions = self.model(bag_tensor): ส่งข้อมูลเข้าสมองกลเพื่อรับคะแนนความมั่นใจของแต่ละ Tag ออกมา\n",
    "            predictions = self.model(bag_tensor)\n",
    "\n",
    "        # torch.argmax(..., dim=1).item(): เลือกตำแหน่ง (Index) ของ Tag ที่ได้คะแนนสูงสุดเพียงอันเดียว\n",
    "        predicted_class_index = torch.argmax(predictions, dim=1).item()\n",
    "\n",
    "        # predicted_intent = self.intents[...]: แปลงตัวเลข Index กลับเป็นชื่อ Tag ที่มนุษย์เข้าใจ (เช่น \"greeting\")\n",
    "        predicted_intent = self.intents[predicted_class_index]\n",
    "\n",
    "        if self.function_mappings:\n",
    "            if predicted_intent in self.function_mappings:\n",
    "                self.function_mappings[predicted_intent]()\n",
    "\n",
    "            if self.intents_response[predicted_intent]:\n",
    "                return random.choice(self.intents_response[predicted_intent])\n",
    "            else:\n",
    "                return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6772b83c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2741031d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "81053d49",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
